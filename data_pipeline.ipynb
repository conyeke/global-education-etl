{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbf19431-9fc0-444c-ab99-50ce262d6e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Extraction Process ---\n",
      "\n",
      "--- Fetching World Bank Data (up to 6 pages) ---\n",
      "Fetched page 1 with 50 records.\n",
      "Fetched page 2 with 50 records.\n",
      "Fetched page 3 with 50 records.\n",
      "Fetched page 4 with 50 records.\n",
      "Fetched page 5 with 50 records.\n",
      "Fetched page 6 with 16 records.\n",
      "Saved 266 total records to school_enrollment.json\n",
      "\n",
      "--- Fetching UNDP Region Index Data ---\n",
      "Successfully fetched UNDP data.\n",
      "Saved records to region_index.json\n",
      "\n",
      "--- Data Extraction Complete ---\n",
      "\n",
      "--- Starting Data Transformation and Merge Process ---\n",
      "Loaded all_countries_df: 250 rows.\n",
      "Transformed region_index_df: 240 rows.\n",
      "Transformed school_enrollment_df: 266 rows.\n",
      "\n",
      "Performing sequential full outer merges...\n",
      "Data transformation and merging complete.\n",
      "\n",
      "--- Final Merged DataFrame Summary ---\n",
      "Shape: (372, 33)\n",
      "\n",
      "First 5 Rows:\n",
      "         country     capital    region     continents       area  population iso2 iso3 countryiso3code  date value unit obs_status  decimal       indicator_id                                                              indicator_value country_id country_value     id                           name  aggregate_projects_count  aggregate_outputs_count  aggregate_budget  aggregate_expense  aggregate_budget_sources country_name country_code country_iso2  country_type country_unit_type    country_donor_lvl country_level_3_code country_mco\n",
      "0          Aruba  Oranjestad  Americas  North America      180.0    106766.0   AW  ABW             ABW  2024  None                      2.0  SE.ENR.PRSC.FM.ZS  School enrollment, primary and secondary (gross), gender parity index (GPI)         AW         Aruba  RBLAC  Latin America & the Caribbean                    1196.0                   1357.0      1.957437e+09        768257482.0                     392.0        Aruba          ABW           AW           2.0                CO  Programme countries                  ABW          TT\n",
      "1    Afghanistan       Kabul      Asia           Asia   652230.0  40218234.0   AF  AFG             AFG  2024  None                      2.0  SE.ENR.PRSC.FM.ZS  School enrollment, primary and secondary (gross), gender parity index (GPI)         AF   Afghanistan   RBAP           Asia and the Pacific                     944.0                   1038.0      8.878313e+08        495208310.0                     274.0  Afghanistan          AFG           AF           2.0                CO  Programme countries                  AFG            \n",
      "2         Angola      Luanda    Africa         Africa  1246700.0  32866268.0   AO  AGO             AGO  2024  None                      2.0  SE.ENR.PRSC.FM.ZS  School enrollment, primary and secondary (gross), gender parity index (GPI)         AO        Angola    RBA                         Africa                    1584.0                   1825.0      1.554101e+09        897571035.0                     250.0       Angola          AGO           AO           2.0                CO  Programme countries                  AGO            \n",
      "3       Anguilla  The Valley  Americas  North America       91.0     13452.0   AI  AIA             NaN   NaN   NaN  NaN        NaN      NaN                NaN                                                                          NaN        NaN           NaN  RBLAC  Latin America & the Caribbean                    1196.0                   1357.0      1.957437e+09        768257482.0                     392.0     Anguilla          AIA           AI           2.0                CO  Programme countries                  AIA          BB\n",
      "4  Ã…land Islands   Mariehamn    Europe         Europe     1580.0     29458.0   AX  ALA             NaN   NaN   NaN  NaN        NaN      NaN                NaN                                                                          NaN        NaN           NaN    NaN                            NaN                       NaN                      NaN               NaN                NaN                       NaN          NaN          NaN          NaN           NaN               NaN                  NaN                  NaN         NaN\n",
      "\n",
      "--- Data Load Complete ---\n",
      "Successfully saved 372 rows to final_merged_data.csv\n",
      "\n",
      "Final Columns after dropping redundant keys:\n",
      "['country', 'capital', 'region', 'continents', 'area', 'population', 'iso2', 'iso3', 'date', 'value', 'unit', 'obs_status', 'decimal', 'indicator_id', 'indicator_value', 'country_id', 'country_value', 'id', 'name', 'aggregate_projects_count', 'aggregate_outputs_count', 'aggregate_budget', 'aggregate_expense', 'aggregate_budget_sources', 'country_name', 'country_iso2', 'country_type', 'country_unit_type', 'country_donor_lvl', 'country_level_3_code', 'country_mco']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "LOCAL_CSV_PATH = 'all_countries.csv'\n",
    "WB_JSON_PATH = 'school_enrollment.json'\n",
    "UNDP_JSON_PATH = 'region_index.json'\n",
    "OUTPUT_CSV_PATH = 'final_merged_data.csv' # New constant for output file\n",
    "WB_BASE_URL = 'https://api.worldbank.org/v2/country/all/indicator/SE.ENR.PRSC.FM.ZS?format=json&date=2024'\n",
    "UNDP_URL = 'https://api.open.undp.org/api/region-index.json'\n",
    "WB_PAGES_LIMIT = 6 # Max pages to fetch\n",
    "\n",
    "def extract_data(\n",
    "    local_csv_path: str = LOCAL_CSV_PATH,\n",
    "    wb_base_url: str = WB_BASE_URL,\n",
    "    wb_pages_limit: int = WB_PAGES_LIMIT,\n",
    "    wb_json_write_path: str = WB_JSON_PATH,\n",
    "    undp_url: str = UNDP_URL,\n",
    "    undp_json_write_path: str = UNDP_JSON_PATH\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Handles fetching data from external APIs and saves API results to local JSON files.\n",
    "    \n",
    "    The local CSV path is included in the return dictionary as it is an input\n",
    "    required for the subsequent transformation step.\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to the extracted (JSON) and input (CSV) files.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Data Extraction Process ---\")\n",
    "    \n",
    "    # NOTE: The local CSV is not read or checked in this function;\n",
    "    # it is assumed to be an existing input file for the transformation step.\n",
    "\n",
    "    # 1. Extract: World Bank API Data with Pagination\n",
    "    print(f\"\\n--- Fetching World Bank Data (up to {wb_pages_limit} pages) ---\")\n",
    "    all_wb_data = []\n",
    "    page = 1\n",
    "    while page <= wb_pages_limit:\n",
    "        params = {'page': page, 'per_page': 50}\n",
    "        try:\n",
    "            response = requests.get(wb_base_url, params=params, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch page {page}: Status {response.status_code}. Breaking loop.\")\n",
    "                break\n",
    "\n",
    "            data = response.json()\n",
    "            # World Bank API returns a list [metadata, data_list]\n",
    "            if len(data) < 2 or not data[1]:\n",
    "                print(f\"No more data on page {page}.\")\n",
    "                break\n",
    "\n",
    "            all_wb_data.extend(data[1])\n",
    "            print(f\"Fetched page {page} with {len(data[1])} records.\")\n",
    "\n",
    "            total_pages = data[0].get('pages', wb_pages_limit)\n",
    "            if page >= total_pages:\n",
    "                break\n",
    "\n",
    "            page += 1\n",
    "            time.sleep(0.5) # Be polite to the API\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error on page {page}: {e}. Stopping World Bank extraction.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during World Bank fetch: {e}\")\n",
    "            break\n",
    "\n",
    "    # Save World Bank data\n",
    "    if all_wb_data:\n",
    "        with open(wb_json_write_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_wb_data, f, indent=4)\n",
    "        print(f\"Saved {len(all_wb_data)} total records to {wb_json_write_path}\")\n",
    "    else:\n",
    "        print(\"No World Bank data fetched to save.\")\n",
    "\n",
    "\n",
    "    # 2. Extract: UNDP API Data (Single Request)\n",
    "    print(\"\\n--- Fetching UNDP Region Index Data ---\")\n",
    "    try:\n",
    "        response = requests.get(undp_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            undp_data = response.json()\n",
    "            print(\"Successfully fetched UNDP data.\")\n",
    "\n",
    "            # Save UNDP data\n",
    "            with open(undp_json_write_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(undp_data, f, indent=4)\n",
    "            print(f\"Saved records to {undp_json_write_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch UNDP data. Status code: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error for UNDP data: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during UNDP fetch: {e}\")\n",
    "\n",
    "    print(\"\\n--- Data Extraction Complete ---\")\n",
    "    return {\n",
    "        'countries_csv': local_csv_path,\n",
    "        'school_enrollment_json': wb_json_write_path,\n",
    "        'region_index_json': undp_json_write_path\n",
    "    }\n",
    "\n",
    "def transform_and_merge_data(file_paths: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads local files, transforms them, and performs a full outer merge.\n",
    "\n",
    "    Args:\n",
    "        file_paths: Dictionary containing paths to the three required data files.\n",
    "\n",
    "    Returns:\n",
    "        A single, fully merged pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Data Transformation and Merge Process ---\")\n",
    "\n",
    "    # --- 1. Load all_countries.csv ---\n",
    "    try:\n",
    "        all_countries_df = pd.read_csv(file_paths['countries_csv'])\n",
    "        print(f\"Loaded all_countries_df: {all_countries_df.shape[0]} rows.\")\n",
    "    except Exception as e:\n",
    "        # This is where the error handling for the missing CSV now resides.\n",
    "        print(f\"ERROR: Could not load countries CSV at {file_paths['countries_csv']}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- 2. Transform region_index.json (Complex Flattening) ---\n",
    "    try:\n",
    "        with open(file_paths['region_index_json'], 'r', encoding='utf-8') as f:\n",
    "            region_data = json.load(f)\n",
    "\n",
    "        # 2a. Normalize base and 'aggregate' keys\n",
    "        df_base = pd.json_normalize(\n",
    "            region_data,\n",
    "            sep='_',\n",
    "            record_path=None,\n",
    "            meta=['id', 'name', 'countries']\n",
    "        )\n",
    "\n",
    "        # 2b. Explode the 'countries' list\n",
    "        df_exploded = df_base.explode('countries').reset_index(drop=True)\n",
    "\n",
    "        # 2c. Flattens the resulting 'countries' column\n",
    "        df_countries = pd.json_normalize(df_exploded['countries']).add_prefix('country_')\n",
    "\n",
    "        # 2d. Combine DataFrames\n",
    "        region_index_df = pd.concat([\n",
    "            df_exploded.drop('countries', axis=1),\n",
    "            df_countries\n",
    "        ], axis=1)\n",
    "        print(f\"Transformed region_index_df: {region_index_df.shape[0]} rows.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to transform region index JSON: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- 3. Transform school_enrollment.json (Simple Flattening) ---\n",
    "    try:\n",
    "        with open(file_paths['school_enrollment_json'], 'r', encoding='utf-8') as f:\n",
    "            school_enrollment_json = json.load(f)\n",
    "\n",
    "        # Normalize the simple nested structure\n",
    "        school_enrollment_df = pd.json_normalize(school_enrollment_json, sep='_')\n",
    "        print(f\"Transformed school_enrollment_df: {school_enrollment_df.shape[0]} rows.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to transform school enrollment JSON: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "    # --- 4. MERGE DATASETS (Full Outer Joins) ---\n",
    "    print(\"\\nPerforming sequential full outer merges...\")\n",
    "\n",
    "    # Define the DataFrames and the required join keys\n",
    "    merge_sequence = [\n",
    "        (school_enrollment_df, 'countryiso3code'),\n",
    "        (region_index_df, 'country_code')\n",
    "    ]\n",
    "\n",
    "    # Use reduce for sequential merging starting with all_countries_df\n",
    "    # Note: 'iso3' is the common column in all_countries_df\n",
    "    df_final = reduce(\n",
    "        lambda left, right_tuple: pd.merge(\n",
    "            left,\n",
    "            right_tuple[0],\n",
    "            left_on='iso3',\n",
    "            right_on=right_tuple[1],\n",
    "            how='outer'\n",
    "        ),\n",
    "        merge_sequence,\n",
    "        all_countries_df\n",
    "    )\n",
    "\n",
    "    print(\"Data transformation and merging complete.\")\n",
    "    return df_final\n",
    "\n",
    "def load_data(df: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the processed DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        output_path: The file path for the output CSV.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Skipping save: DataFrame is empty.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # index=False prevents pandas from writing the DataFrame index as a column\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        print(f\"\\n--- Data Load Complete ---\")\n",
    "        print(f\"Successfully saved {df.shape[0]} rows to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save DataFrame to CSV: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Execute the Data Pipeline ---\n",
    "    \n",
    "    # Note: To run this successfully, a file named 'all_countries.csv' \n",
    "    # must exist in the same directory, containing an 'iso3' column.\n",
    "    \n",
    "    # 1. Extraction: Fetch API data and save files\n",
    "    file_paths = extract_data()\n",
    "    \n",
    "    # 2. Transformation and Merging: Load files and combine them\n",
    "    final_df = transform_and_merge_data(file_paths)\n",
    "    \n",
    "    if not final_df.empty:\n",
    "        print(\"\\n--- Final Merged DataFrame Summary ---\")\n",
    "        print(f\"Shape: {final_df.shape}\")\n",
    "        print(\"\\nFirst 5 Rows:\")\n",
    "        # Use to_string() to ensure full output is visible in the console\n",
    "        print(final_df.head().to_string())\n",
    "        \n",
    "        # 3. Loading: Save the final DataFrame to the configured path\n",
    "        load_data(final_df, OUTPUT_CSV_PATH)\n",
    "        \n",
    "        # Drop redundant merge key columns\n",
    "        final_df = final_df.drop(columns=['countryiso3code', 'country_code'])\n",
    "        print(\"\\nFinal Columns after dropping redundant keys:\")\n",
    "        print(final_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d4b8f-dfca-477f-80ac-24ddb648b6db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
